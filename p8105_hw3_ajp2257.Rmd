---
title: "HW3"
author: "Amy Pitts"
date: "10/10/2020"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggridges)
library(p8105.datasets)
library(patchwork)
```


# Problem 1 

This problem uses the Instacart data. DO NOT include this dataset in your local data directory; instead, load the data from the p8105.datasets using:
```{r}
data("instacart")
```

The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and giving illstrative examples of observations. Then, do or answer the following (commenting on the results of each):



```{r}
head(instacart)
```

This dataset contains `r nrow(instacart)` rows and `r nrow(instacart)` columns. This data is all about instacart orders. 
Observations are at the level of items in orders by users of the insta cart app. There are user variables/ order variables, user ID, order ID, order data, and order hour. There are also item variables -- name, aisle, department, and some numeric codes. 

Also should contain the structure of the data. For example the aisle is realte dto the department. Also the aisle name is numberic and also is the same as the aisle id. 



- How many aisles are there, and which aisles are the most items ordered from?

```{r}
instacart %>%
  count(aisle) %>%
  arrange(desc(n))
```
The most things are coming from fresh vegetables and fresh friuts. 


- Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.

Lets make a plot 
```{r}
instacart %>%
  count(aisle) %>%
  filter(n > 10000) %>%
  mutate (
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n) #re-order isle according to n
  ) %>%
  ggplot(aes(x = aisle, y = n)) +
  geom_point() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) #needed to tilt the words 
```


- Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>%
  count(product_name) %>%
  mutate(rank = min_rank(desc(n))) %>%
  filter(rank < 4) %>%
  arrange(desc(n)) %>%
  knitr::kable()
```



- Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

```{r}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize( mean_hour = mean(order_hour_of_day)) %>%
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) %>%
  knitr::kable()
```



# Problem 2

Accelerometers have become an appealing alternative to self-report techniques for studying physical activity in observational studies and clinical trials, largely because of their relative objectivity. During observation periods, the devices measure “activity counts” in a short period; one-minute intervals are common. Because accelerometers can be worn comfortably and unobtrusively, they produce around-the-clock observations.

This problem uses five weeks of accelerometer data collected on a 63 year-old male with BMI 25, who was admitted to the Advanced Cardiac Care Center of Columbia University Medical Center and diagnosed with congestive heart failure (CHF). The data can be downloaded here. In this spreadsheet, variables activity.* are the activity counts for each minute of a 24-hour day starting at midnight.

```{r data_q2}
activity = read_csv(file = "data/accel_data.csv") %>%
  janitor::clean_names()
```

- Load, tidy, and otherwise wrangle the data. Your final dataset should include all originally observed variables and values; have useful variable names; include a weekday vs weekend variable; and encode data with reasonable variable classes. Describe the resulting dataset (e.g. what variables exist, how many observations, etc).


We need to pivot longer so that all the activates do not have there own column. Currently there is `r nrow(activity)` rows and `r ncol(activity)` columns which is a lot. The idea with pivot longer is to reduce the number of columns and make more rows. We are also going to create another `week_end_vs_day` variable that tells us if the day is during the week or on the weekend. We also force the variables to be the correct type. 

```{r}
activity_long = 
  pivot_longer(
    activity, 
    activity_1:activity_1440,
    names_to = "activity_number", 
    names_prefix = "activity_",
    values_to = "activity_minute") %>%
  mutate(
    activity_minute = as.numeric(activity_minute),
    day = factor(day),
    activity_number = as.numeric(activity_number),
    week_end_vs_day = ifelse(day_id == c(3,4), "weekend", "weekday")
  )

head(activity_long)
```

This results in a database with `r nrow(activity_long)` rows and `r ncol(activity_long)` columns which makes this have a lot more rows then columns than before. 

- Traditional analyses of accelerometer data focus on the total activity over the day. Using your tidied dataset, aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?

```{r}
activity_long %>%
  group_by( week, day) %>%
  mutate (
    day = factor(day, levels= c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))
  ) %>%
  summarize(total_activity = sum(activity_minute)) %>%
  pivot_wider(
    names_from = day,
    values_from = total_activity
  ) %>%
  knitr::kable()
```

Here just looking at the numbers it is hard to see very distinct patterns. A couple of things I notice is that on the last two Saturday the person barely has any movement which is a big difference compared to other days. Also on Wednesdays through all the weeks he looks to have around the same amount of activity comparatively.  



- Accelerometer data allows the inspection activity over the course of the day. Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.

```{r q2_plot}
activity_long %>%
  ggplot(aes(x = as.numeric(activity_number), y = as.numeric(activity_minute), color=day)) +
  geom_smooth(se = FALSE) +
  scale_y_continuous(limit=c(0,450)) +
  labs( 
    title = "24-hour activity time by day of the Week",
    x = "1440 Minute in the day (0 is Midnight)",
    y = "Activity Count"
    #caption = "Data looking at the Activity by minute"
  ) 

```


Looking at the graph we see that at the very beginning and the very end there is less activity which makes sense because that is closest to midnight so the person is probably asleep. Also, most activity is in the middle of the graph which makes sense because that is the beginning of the day. The lest activity seems to be on Saturdays which makes sense with the table from above because he did have a couple weeks of very low activity on Saturdays. 




# Problem 3 
This problem uses the NY NOAA data. DO NOT include this dataset in your local data directory; instead, load the data from the p8105.datasets package using:

```{r}
data("ny_noaa") 
```

The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and indicating the extent to which missing data is an issue. Then, do or answer the following (commenting on the results of each):

This data focuses on New York state from Jan 1, 1981 to Dec 31, 2010 and looks at the snowfall. The dataset has `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. The variables of interest are the id, data which hold the year, month and day, the `prcp` which represents the precipitations, the `snow`, `snwd`, `tmax`, and `tmin`.  Quickly glancing at the head of the data we see a lot of missing values in the weather related data. 

```{r}
head(ny_noaa)
```
```{r}
ny_noaa %>%
  gather(key = "key", value = "val") %>%
  mutate(is.missing = is.na(val)) %>%
  group_by(key, is.missing) %>%
  summarise(num.missing = n()) %>%
  filter(is.missing==T) %>%
  select(-is.missing) %>%
  arrange(desc(num.missing)) %>%
  knitr::kable()
```
This table shows us that there is a lot of missing values in the dataset. The most in the `tmax` and `tmin` and the least in the `prcp`. The date and the ID are not included in this table becuase they do not have missing values. 


- Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. For snowfall, what are the most commonly observed values? Why?

We are first cleaning up the data. This involves putting the date variable into a year, month, and day variables. We also want to force the max and min temperature to be numeric values instead of characters since that makes more sense with what temperature is. 
```{r}
ny_noaa_fixed = ny_noaa %>%
  separate(date, sep="-", into = c("year", "month", "day")) %>%
  mutate(
   year = as.integer(year),
   month = as.integer(month),
   day = as.integer(day),
   tmax = as.numeric(tmax),
   tmin = as.numeric(tmin)
  )
 
```

Looking at the most common snow fall value. 
```{r}
ny_noaa_fixed %>%
  count(snow) %>%
  arrange(desc(n)) 
```

The most common value of the snow variable is 0. This makes sense because the snow only fall for less then half the year in NY because NY experiences all 4 seasons. Thus when it is summer, early fall, or late spring there is less likely to be a chance to have snow. The second common values is NA or a missing value. 



- Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

```{r}
ny_noaa_fixed %>%
  filter(month == c(1,7)) %>%
  group_by(year, month) %>%
  summarise( mean_temp = mean(tmax, na.rm = TRUE)) %>%
  mutate(
    month = ifelse( month == 1, "January", "July")
  ) %>%
  filter(mean_temp != "NaN") %>%
    ggplot(aes(x = year, y = mean_temp)) +
      geom_point(aes(color = factor(month))) + 
      theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
      facet_grid(.~month ) +
      labs(
        title = "Mean Max Tempature in NY Over the Years",
        x = "Year (1980 - 2010)",
        y = "Mean daily temperature (C)"
      ) +
      scale_color_hue(name = "Month", h = c(100, 300))



  
```

```{r}
ny_noaa_fixed %>%
  filter(month == c(1,7)) %>%
  select(year, month, tmax) %>%
  group_by(year, month) %>%
  mutate(
    month = ifelse( month == 1, "January", "July")
  ) %>%
  filter(tmax != "NA") %>%
  ggplot( aes(x = factor(year), y = tmax)) + 
    geom_violin(aes(fill = month), alpha = .5) + 
    stat_summary(fun = "mean", color = "black") +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
    facet_grid(month ~.) +
    labs(
        title = "NY Weather Over the Years Stratified by Months",
        x = "Year (1980 - 2010)",
        y = "Maxiumum daily temperature (C)"
      ) 

```

From the two plots above we see that in January the average max temp is much lower then in July which is to be expected since it is summer in July in NY and winter in January. From each month throughout the years all the temps seem to be close to the same which a couple of outliers. In July  the years are 1989, 1991, and 2006 where the outliers are much cooler temps.  In Jan year 2005 has a very very warm which is a outlier. 

- Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r}
one = ggplot(ny_noaa_fixed, aes(x = tmax, y = tmin)) + 
  geom_bin2d() +
  labs(
    title = "Min and Max Tempatures in NY",
    x = "Max daily temperature (C)",
    y = "Min daily temperature (C)"
  ) 


two = ny_noaa_fixed %>%
  select(year, snow) %>%
  group_by(year)%>%
  filter(snow > 0, snow < 100) %>%
  ggplot(aes(x=factor(year), y=snow)) +
    geom_boxplot(fill = "lightblue") +
    stat_summary(fun = "mean", color = "blue") +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
    labs(
          title = "NY Snow Fall",
          x = "Years",
          y = "Snow Fall"
        ) 
  
(one / two)
```

It is really interesting that the median is the same for snow fall all the years. However, the mean is different for all the year which can help show the years that had more extreme snowfalls. 










